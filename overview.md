# HPC Project Overview â€” Comparative Study of Parallel Paradigms

## ðŸŽ¯ Project Aim
Design, implement, and analyze high-performance matrix-multiplication algorithms across **three major parallel architectures** â€”  
**Shared-Memory (OpenMP)**, **Distributed-Memory (MPI)**, and **GPU (CUDA)** â€” to understand how computation, communication, and memory design impact scalability and efficiency.

---

## ðŸ§© Motivation
Modern scientific and data-intensive applications rely on scalable computing.  
Each HPC model â€” OpenMP, MPI, and CUDA â€” tackles parallelism differently:

| Model | Memory Type | Communication | Typical Scale |
|--------|--------------|----------------|----------------|
| **Sequential** | Single core | None | Baseline |
| **OpenMP** | Shared memory | Implicit | Multi-core CPU |
| **MPI** | Distributed memory | Explicit (message passing) | Multi-node clusters |
| **CUDA / GPU** | Device global/shared memory | Hostâ€“device | Thousands of GPU threads |

This project builds layer-by-layer understanding of these paradigms, measuring **speedup, efficiency, and scalability** at each stage.

---

## ðŸ§± Architecture of the Study

### **Phase 1 â€“ Day 1: Sequential & OpenMP (Shared-Memory Parallelism)**
**Goal:** Establish CPU-based baseline and measure intra-node scaling.

- Implemented `sequential_matmul.cpp` and `openmp_matmul.cpp`.
- Benchmarked with 1, 2, 4, 6, 12 threads for N = 500â€“2000.
- Used `#pragma omp parallel for` for loop parallelization.
- Recorded results â†’ `day1_basic.csv`, summarized â†’ `day1_summary_stats.csv`.

**Key Findings**
| Metric | Observation |
|---------|--------------|
| Speedup | Near-linear up to 6 threads (â‰ˆ physical cores) |
| Efficiency | â‰ˆ 90 % up to 6 threads, falls to ~70 % at 12 threads |
| Overhead | Thread creation + scheduling costs visible for small N |
| Verification | Checksums consistent with sequential baseline |

**Conclusion:**  
OpenMP achieves strong scaling within one CPU socket, but saturates once shared cache and memory bandwidth become limiting.

---

### **Phase 2 â€“ Day 2: MPI (Distributed-Memory Parallelism)**
**Goal:** Break shared-memory limits and analyze process-level scaling.

- Implemented `mpi_matmul.cpp` using **MPI 4.1.6**.  
- Partitioned matrix A across processes, broadcast B (`MPI_Bcast`), gathered C (`MPI_Gatherv`).  
- Benchmarked N = 500â€“4000, processes = 1, 2, 4, 6 Ã— 5 repeats.  
- Added process binding (`--bind-to core`) for stable timings.  
- Automated experiments via `run_day2_mpi_tests.sh`.

**Key Findings**
| Matrix N | Processes | Mean Time (s) | Speedup | Efficiency (%) |
|-----------:|------------:|--------------:|---------:|----------------:|
| 500 | 1â†’6 | 0.110â†’0.038 | 2.9Ã— | 80â€“90 |
| 1000 | 1â†’6 | 1.44â†’1.37 | 1.1Ã— | 20â€“40 |
| 2000 | 1â†’6 | 27.3â†’11.3 | 2.4Ã— | 40â€“55 |

**Interpretation**
- Small N: Communication > Computation â†’ weak scaling.  
- Large N: Computation dominates â†’ â‰ˆ 2.5Ã— speedup at 6 processes.  
- Amdahlâ€™s fit f â‰ˆ 0.9 (90 % parallel fraction).  
- Efficiency drops after 4 processes due to memory and sync overhead.

**Conclusion:**  
MPI scales beyond OpenMP for larger workloads but introduces measurable message-passing cost.  
It prepares the ground for hybrid (MPI + OpenMP) and GPU-based distributed experiments.

---

### **Phase 3 â€“ Day 3 (Upcoming): GPU Acceleration (CUDA/cuBLAS)**
**Goal:** Explore device-level parallelism using NVIDIA RTX 3050 GPU (4 GB VRAM, CUDA 12.7).

**Planned Work**
1. Implement `gpu_matmul.cu` using **CUDA kernels** and **cuBLAS DGEMM**.  
2. Benchmark vs CPU baselines (Sequential / OpenMP / MPI).  
3. Collect metrics: GPU-utilization, kernel latency, PCIe transfer time.  
4. Analyze **compute vs memory bound regions** and **energy efficiency**.

**Expected Outcome**
- GPU expected to outperform CPU > 10Ã— for N â‰¥ 2000.  
- Performance limited by VRAM capacity and host-device transfer overhead.  
- Completes the 3-tier scalability curve:  
  `CPU (Threads) â†’ Processes â†’ GPU Kernels`.

---

## âš™ï¸ Benchmarking and Analysis Methodology
| Step | Technique | Purpose |
|------|------------|----------|
| **Multiple Repeats** | 5 runs per config | Reduce noise & average out variance |
| **Core Binding** | `--bind-to core` | Ensure process/thread affinity |
| **Timing** | `std::chrono` / `MPI_Wtime()` / CUDA Events | Accurate wall-clock measurement |
| **Validation** | Checksum comparison | Functional correctness |
| **Statistical Summary** | Mean Â± Std Dev | Measurement stability |
| **Visualization** | Matplotlib | Trends (Speedup, Efficiency, Scaling) |

---

## ðŸ§® Key Takeaways So Far
| Domain | Lesson |
|---------|---------|
| **Performance Modeling** | Learned Amdahlâ€™s Law and efficiency drop with communication. |
| **Resource Monitoring** | Used `htop`, `nvidia-smi`, `free -h` for system-level profiling. |
| **Experimental Rigor** | Built repeatable scripts + CSV logging framework. |
| **Scaling Behavior** | CPU (OpenMP) good up to cores; MPI good for larger data; GPU next for massive parallelism. |

---

## ðŸ§¾ Deliverables So Far
| File | Description |
|------|--------------|
| `openmp/src/openmp_matmul.cpp` | Shared-memory implementation |
| `openmp/results/day1_summary_stats.csv` | Baseline results |
| `mpi/src/mpi_matmul.cpp` | Distributed-memory implementation |
| `mpi/scripts/run_day2_mpi_tests.sh` | MPI benchmark script |
| `mpi/results/day2_mpi_summary.csv` | Aggregated MPI data |
| `docs/day1_summary.md` | OpenMP report |
| `docs/day2_summary.md` | MPI report |
| `docs/overview.md` | This master overview document |

---

## ðŸš€ Research Roadmap
| Stage | Focus | Deliverable |
|--------|--------|-------------|
| âœ… Day 1 | Sequential & OpenMP benchmarking | `day1_summary.md` |
| âœ… Day 2 | MPI scaling and distributed analysis | `day2_summary.md` |
| ðŸ”œ Day 3 | GPU (CUDA/cuBLAS) acceleration | `gpu_summary.md` |
| ðŸ”œ Day 4 (optional) | Hybrid (MPI + OpenMP + CUDA) | `hybrid_summary.md` |
| ðŸ“„ Final | Paper + arXiv submission | `paper_final.pdf` |

---

## ðŸ§  Grand Understanding
> **Sequential â†’ OpenMP â†’ MPI â†’ CUDA**  
> mirrors the real-world evolution of parallel computing â€” from single core to clusters to accelerators.  
> Each layer teaches how computation, communication, and hardware architecture shape performance scaling.

---

*Prepared by Gaurav â€” MS CS, NJIT (2025)*  
*High-Performance Computing Research Project â€” Parallel Systems & Scalability Analysis*